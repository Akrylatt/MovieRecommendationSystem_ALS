{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Machine Learning and Deep Learning - Assignment 2 - Movie Recommender System\n",
        "\n",
        "## Task description\n",
        "\n",
        "A recommender system is a type of information filtering system that suggests items or content to users based on their interests, preferences, or past behavior. These systems are commonly used in various domains, such as e-commerce, entertainment, social media, and online content platforms.\n",
        "\n",
        "Your assignment is to create a recommender system of movies for users:\n",
        "* Your system should suggest some movies to the user based on user's gemographic information(age, gender, occupation, zip code) and favorite movies (list of movie ids).\n",
        "* Solve this task using a machine learning model. You may consider only one model: it will be enough.\n",
        "* Create a benchmark that would evaluate the quality of recommendations of your model. Look for commonly used metrics to evaluate a recommender system and use at least one metric.\n",
        "* Make a single report decribing data exploration, solution implementation, training process, and evaluation on the benchmark.\n",
        "* Explicitly state the benchmark scores of your systems.\n",
        "\n",
        "Submission should be a link to GitHub repository. It should be open repository, so that the instructors could assess it easily.\n",
        "\n",
        "## Data Description\n",
        "\n",
        "In this assignment you will use [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) consisting user ratings to movies.\n",
        "\n",
        "**General information about the dataset:**\n",
        "* It consists of 100,000 ratings from 943 users on 1682 movies\n",
        "* Ratings are ranged from 1 to 5\n",
        "* Each user has rated at least 20 movies\n",
        "* It contains simple demographic info for the users (age, gender, occupation, zip code)\n",
        "\n",
        "**Detailed description of data files:**\n",
        "\n",
        "| **File** | **Description** |\n",
        "| -------- | --------------- |\n",
        "| u.data | Full dataset of 100000 ratings by 943 users on 1682 items. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id, item id, rating, and timestamp. The time stamps are unix seconds. |\n",
        "| u.info | The number of users, items, and ratings in the u data set |\n",
        "| u.item | Information about the items (movies). This is a tab separated list of movie id, movie title, release date, video release date, IMDB URL, and genres. The last 19 fields are genres and contain binary values. Movies can be of several genres at once. The movie ids are the ones used in u.data |\n",
        "| u.genre | List of genres. |\n",
        "| u.user | Demographic information about the users. This is a tab separated list of user id, age, gender, occupation, zip code. The user ids are the ones used in in u.data file. |\n",
        "| u.occupation | List of occupations. |\n",
        "| u1.base, u1.test, u2.base, u2.test, u3.base, u3.test, u4.base, u3.test, u5.base, u5.test | The data sets u1.base and u1.test through u5.base and u5.test are 80%/20% splits of the u data into training and test data. Each of u1, ..., u5 have disjoint test sets; this if for 5 fold cross validation (where you repeat your experiment with each training and test set and average the results). These data sets can be generated from u.data by mku.sh. |\n",
        "| ua.base, ua.test, ub.base, ub.test | The data sets ua.base, ua.test, ub.base, and ub.test split the u data into a training set and a test set with exactly 10 ratings per user in the test set. The sets ua.test and ub.test are disjoint. These data sets can be generated from u.data by mku.sh. |\n",
        "| allbut.pl | The script that generates training and test sets where all but n of a users ratings are in the training data |\n",
        "| mku.sh | A shell script to generate all the u data sets from u.data. |\n",
        "\n",
        "## Evaluation criterias\n",
        "\n",
        "The repository should have the following structure:\n",
        "\n",
        "```\n",
        "movie-recommender-system\n",
        "├── README.md               # The top-level README\n",
        "│\n",
        "├── data\n",
        "│   ├── external            # Data from third party sources\n",
        "│   ├── interim             # Intermediate data that has been transformed.\n",
        "│   └── raw                 # The original, immutable data\n",
        "│\n",
        "├── models                  # Trained and serialized models, final checkpoints\n",
        "│\n",
        "├── notebooks               #  Jupyter notebooks. Naming convention is a number (for ordering),\n",
        "│                               and a short delimited description, e.g.\n",
        "│                               \"1.0-initial-data-exporation.ipynb\"            \n",
        "│\n",
        "├── references              # Data dictionaries, manuals, and all other explanatory materials.\n",
        "│\n",
        "├── reports\n",
        "│   ├── figures             # Generated graphics and figures to be used in reporting\n",
        "│   └── final_report.pdf    # Report containing data exploration, solution exploration, training process, and evaluation\n",
        "│\n",
        "└── benchmark\n",
        "    ├── data                # dataset used for evaluation\n",
        "    └── evaluate.py         # script that performs evaluation of the given model\n",
        "```\n",
        "\n",
        "\n",
        "In the top `README.md` file put your name, email and group number.\n",
        "\n",
        "In the `reports` directory create a report about your work. In the report, describe in details the implementation of your system. Mention its advantages and disadvantages.\n",
        "\n",
        "### Expected Report Structure\n",
        "\n",
        "```\n",
        "# Introduction\n",
        "...\n",
        "# Data analysis\n",
        "...\n",
        "# Model Implementation\n",
        "...\n",
        "# Model Advantages and Disadvantages\n",
        "...\n",
        "# Training Process\n",
        "...\n",
        "# Evaluation\n",
        "...\n",
        "# Results\n",
        "...\n",
        "```\n",
        "\n",
        "In the `notebooks` directory put at least two notebooks. **First notebook** should contain your initial data exploration and basic ideas behind data preprocessing. **Second notebook** should contain information about final solution training and visualization.\n",
        "\n",
        "## Grading criterias\n",
        "\n",
        "Full assignment without any problems is said to be the `100%` solution.\n",
        "\n",
        "| Criteria | Weight (%) | Comment |\n",
        "| ---- | ----- | ----- |\n",
        "| Structure and code quality | 30 | Code quality, structure, comments, clean repo, commit history, reproducibility (manual seeding) |\n",
        "| Visualization, notebooks quality | 10 | Jupyter notebooks, visualizations |\n",
        "| Solution building | 40 |  Implementation description, references, final report structure |\n",
        "| Final score, evaluation  | 20 | Evaluation function, final score, quality of results |\n",
        "\n",
        "If **PMLDL Course Team** will have any questions about your assignment or your work fails to show your results you will be called solution defence procedure.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VLspCCnF2J7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution"
      ],
      "metadata": {
        "id": "0GFjBdxy2SYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data downloading"
      ],
      "metadata": {
        "id": "qpFz6w7W2bND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complete_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip'\n",
        "small_dataset_url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'"
      ],
      "outputs": [],
      "metadata": {
        "id": "O7TFEMz5OTbO"
      },
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S5iNV2KhYGa",
        "outputId": "f5310ab5-d95f-4a70-f125-40ae47c2d063"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=15afc752c6b229803d6454e91fd85c8ee768043486945efc992b9b9b71daafe5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import wget\n",
        "\n",
        "complete_f = wget.download(complete_dataset_url)\n",
        "\n",
        "small_f = wget.download(small_dataset_url)"
      ],
      "outputs": [],
      "metadata": {
        "id": "_9nChB8bOTbV"
      },
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ml-latest-small.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-latest-small.zip\n",
            "   creating: ml-latest-small/\n",
            "  inflating: ml-latest-small/links.csv  \n",
            "  inflating: ml-latest-small/tags.csv  \n",
            "  inflating: ml-latest-small/ratings.csv  \n",
            "  inflating: ml-latest-small/README.txt  \n",
            "  inflating: ml-latest-small/movies.csv  \n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XerNxFROTba",
        "outputId": "67ca3720-7736-4dc8-9121-ffd9e76bff75"
      },
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ml-latest.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdX9L7v_iZmr",
        "outputId": "c25c656a-73cc-4e8d-b1b4-f8c646851ea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ml-latest.zip\n",
            "   creating: ml-latest/\n",
            "  inflating: ml-latest/tags.csv      \n",
            "  inflating: ml-latest/links.csv     \n",
            "  inflating: ml-latest/README.txt    \n",
            "  inflating: ml-latest/ratings.csv   \n",
            "  inflating: ml-latest/genome-tags.csv  \n",
            "  inflating: ml-latest/genome-scores.csv  \n",
            "  inflating: ml-latest/movies.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpvgnPbrmkNg",
        "outputId": "e0b7a0e6-7e61-4990-ae43-15f657dcfbdf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=eb92fe757f69adaa6d11d4b6033f2dce648a266c5d482ab53c450b36f63ecc18\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicializiting Spark"
      ],
      "metadata": {
        "id": "MFEBypkU2eHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "t9zNJmz2oYYX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext()"
      ],
      "metadata": {
        "id": "uSHI3vJFoi5m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I will use the smaller version of the dataset for learning the optimal parapeters\n",
        "small_ratings_file = \"/content/ml-latest-small/ratings.csv\"\n",
        "\n",
        "small_ratings_raw_data = sc.textFile(small_ratings_file)\n",
        "small_ratings_raw_data_header = small_ratings_raw_data.take(1)[0]"
      ],
      "metadata": {
        "id": "-JfBnsDKuBSo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_ratings_data = small_ratings_raw_data.filter(lambda line: line!=small_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()"
      ],
      "metadata": {
        "id": "LC9qhqeGtqDg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_ratings_data.take(3)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', '1', '4.0'), ('1', '3', '4.0'), ('1', '6', '4.0')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "metadata": {
        "id": "gvWj3RbkOTbr",
        "outputId": "573e7e71-ddf8-4db5-8037-7e1e0ba18166",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the data, we use pyspark rdd\n",
        "datasets_path = \"/content/\"\n",
        "\n",
        "small_movies_file = os.path.join(datasets_path, 'ml-latest-small', 'movies.csv')\n",
        "\n",
        "small_movies_raw_data = sc.textFile(small_movies_file)\n",
        "small_movies_raw_data_header = small_movies_raw_data.take(1)[0]\n",
        "\n",
        "small_movies_data = small_movies_raw_data.filter(lambda line: line!=small_movies_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
        "\n",
        "small_movies_data.take(3)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('1', 'Toy Story (1995)'),\n",
              " ('2', 'Jumanji (1995)'),\n",
              " ('3', 'Grumpier Old Men (1995)')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "metadata": {
        "id": "s33pBfkPOTbw",
        "outputId": "fc2f67dd-566b-4352-e10d-28ea3b7f5231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Selecting ALS parameters using the small dataset"
      ],
      "metadata": {
        "id": "VUP9Ho1gOTb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the datasets\n",
        "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0)\n",
        "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
        "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"
      ],
      "outputs": [],
      "metadata": {
        "id": "eKnS9weZOTb2"
      },
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import ALS\n",
        "import math\n",
        "\n",
        "seed = 5\n",
        "iterations = 10\n",
        "regularization_parameter = 0.1\n",
        "ranks = [4, 8, 12]\n",
        "errors = [0, 0, 0]\n",
        "err = 0\n",
        "tolerance = 0.02\n",
        "\n",
        "min_error = float('inf')\n",
        "best_rank = -1\n",
        "best_iteration = -1\n",
        "for rank in ranks:\n",
        "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
        "                      lambda_=regularization_parameter)\n",
        "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "    errors[err] = error\n",
        "    err += 1\n",
        "    print ('For rank %s the RMSE is %s' % (rank, error))\n",
        "    if error < min_error:\n",
        "        min_error = error\n",
        "        best_rank = rank\n",
        "\n",
        "print ('The best model was trained with rank %s' % best_rank)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For rank 4 the RMSE is 0.9121002117526161\n",
            "For rank 8 the RMSE is 0.9184327220254898\n",
            "For rank 12 the RMSE is 0.9160151516811809\n",
            "The best model was trained with rank 4\n"
          ]
        }
      ],
      "metadata": {
        "id": "EPFjLLSYOTb4",
        "outputId": "adef45a4-31b9-4a0d-997d-cda810e93815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the predictions\n",
        "model = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n",
        "                      lambda_=regularization_parameter)\n",
        "predictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "\n",
        "print('For testing data the RMSE is %s' % (error))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For testing data the RMSE is 0.912132604158584\n"
          ]
        }
      ],
      "metadata": {
        "id": "mB1wLwbeOTcA",
        "outputId": "0a450fdb-19d1-466e-8018-b72c676784f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using the complete dataset to build the final model"
      ],
      "metadata": {
        "id": "lSWyRApVOTcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the complete dataset file\n",
        "complete_ratings_file = os.path.join(datasets_path, 'ml-latest', 'ratings.csv')\n",
        "complete_ratings_raw_data = sc.textFile(complete_ratings_file)\n",
        "complete_ratings_raw_data_header = complete_ratings_raw_data.take(1)[0]\n",
        "\n",
        "# Parse\n",
        "complete_ratings_data = complete_ratings_raw_data.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()\n",
        "\n",
        "print (\"There are %s recommendations in the complete dataset\" % (complete_ratings_data.count()))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 33832162 recommendations in the complete dataset\n"
          ]
        }
      ],
      "metadata": {
        "id": "LGJCYxK4OTcD",
        "outputId": "b05c7b28-7bb6-4aff-e7f9-1c3050c39233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": [
        "# training the final dataset\n",
        "training_RDD, test_RDD = complete_ratings_data.randomSplit([7, 3], seed=0)\n",
        "\n",
        "complete_model = ALS.train(training_RDD, best_rank, seed=seed,\n",
        "                           iterations=iterations, lambda_=regularization_parameter)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hFzaj6P8OTcE"
      },
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the final model\n",
        "test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "predictions = complete_model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
        "rates_and_preds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
        "error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
        "\n",
        "print ('For testing data the RMSE is %s' % (error))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For testing data the RMSE is 0.8261327073910048\n"
          ]
        }
      ],
      "metadata": {
        "id": "BI_jfevZOTcG",
        "outputId": "9b42a299-bd34-4dc7-e4d4-1f53a6a80543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting recommendation by myself"
      ],
      "metadata": {
        "id": "tA5ysk-l7dn5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make a prediction for a random user with such a history:\n",
        "\n",
        "userId,movieId,rating,timestamp\n",
        "\n",
        "1,19,4.0,964982703\n",
        "\n",
        "1,34,4.0,964981247\n",
        "\n",
        "1,6,4.0,964982224\n",
        "\n",
        "1,47,5.0,964983815\n",
        "\n",
        "1,51,5.0,964982931\n",
        "\n",
        "1,75,3.0,964982400\n",
        "\n",
        "1,10,5.0,964980868\n",
        "\n",
        "1,10,4.0,964982176\n",
        "\n",
        "1,151,5.0,964984041\n",
        "\n",
        "1,117,5.0,964984100\n",
        "\n",
        "1,13,5.0,964983650\n",
        "\n",
        "1,2,5.0,964981208\n",
        "\n",
        "1,23,3.0,964980985\n"
      ],
      "metadata": {
        "id": "TAOo1peu0mNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "complete_movies_file = os.path.join(datasets_path, 'ml-latest', 'movies.csv')\n",
        "complete_movies_raw_data = sc.textFile(complete_movies_file)\n",
        "complete_movies_raw_data_header = complete_movies_raw_data.take(1)[0]\n",
        "\n",
        "# Parse\n",
        "complete_movies_data = complete_movies_raw_data.filter(lambda line: line!=complete_movies_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),tokens[1],tokens[2])).cache()\n",
        "\n",
        "complete_movies_titles = complete_movies_data.map(lambda x: (int(x[0]),x[1]))\n",
        "\n",
        "print (\"There are %s movies in the complete dataset\" % (complete_movies_titles.count()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVdG2Qm6z7h-",
        "outputId": "e9d9fc14-e029-45ee-b64e-9f4e48169927"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 86537 movies in the complete dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_counts_and_averages(ID_and_ratings_tuple):\n",
        "    nratings = len(ID_and_ratings_tuple[1])\n",
        "    return ID_and_ratings_tuple[0], (nratings, float(sum(x for x in ID_and_ratings_tuple[1]))/nratings)\n",
        "\n",
        "movie_ID_with_ratings_RDD = (complete_ratings_data.map(lambda x: (x[1], x[2])).groupByKey())\n",
        "movie_ID_with_avg_ratings_RDD = movie_ID_with_ratings_RDD.map(get_counts_and_averages)\n",
        "movie_rating_counts_RDD = movie_ID_with_avg_ratings_RDD.map(lambda x: (x[0], x[1][0]))"
      ],
      "metadata": {
        "id": "GMT5HI4q0B4G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_ratings_file_eval = \"/content/eval/ratings.csv\"\n",
        "raw_eval = sc.textFile(complete_ratings_file_eval)"
      ],
      "metadata": {
        "id": "frjAvc15Fyhp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_eval = raw_eval.filter(lambda line: line!=complete_ratings_raw_data_header)\\\n",
        "    .map(lambda line: line.split(\",\")).map(lambda tokens: (int(tokens[0]),int(tokens[1]),float(tokens[2]))).cache()"
      ],
      "metadata": {
        "id": "OQpzhVNVFaej"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_to_recommend = data_eval.map(lambda x: (x[0], x[1]))\n",
        "new_user_recommendations_RDD = complete_model.predictAll(users_to_recommend)"
      ],
      "metadata": {
        "id": "z3s_osJV_oG3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_user_recommendations_rating_RDD = new_user_recommendations_RDD.map(lambda x: (x.product, x.rating))\n",
        "new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_RDD.join(complete_movies_titles).join(movie_rating_counts_RDD)\n",
        "new_user_recommendations_rating_title_and_count_RDD.take(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7j4Gv4IsAUVE",
        "outputId": "f7a17b9f-9dce-4d99-ad23-5934fb6d31db"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(97328, ((3.2456643250798933, 'Liberal Arts (2012)'), 414)),\n",
              " (4928,\n",
              "  ((4.1395190947861344,\n",
              "    'That Obscure Object of Desire (Cet obscur objet du désir) (1977)'),\n",
              "   905)),\n",
              " (4928,\n",
              "  ((3.9225236849812015,\n",
              "    'That Obscure Object of Desire (Cet obscur objet du désir) (1977)'),\n",
              "   905))]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_user_recommendations_rating_title_and_count_RDD = new_user_recommendations_rating_title_and_count_RDD.map(lambda r: (r[1][0][1], r[1][0][0], r[1][1]))"
      ],
      "metadata": {
        "id": "ZFJ-iAXnAYfv"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting the recommendations in human-friendly shape"
      ],
      "metadata": {
        "id": "2QnkYmgL3tS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_movies = new_user_recommendations_rating_title_and_count_RDD.filter(lambda r: r[2]>=25).takeOrdered(25, key=lambda x: -x[1])\n",
        "\n",
        "print ('TOP recommended movies (with more than 25 reviews):\\n%s' %\n",
        "        '\\n'.join(map(str, top_movies)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euugTGqTAdcZ",
        "outputId": "e32a6585-c104-4995-866c-6c05b079f3fc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOP recommended movies (with more than 25 reviews):\n",
            "('Apocalypse Now (1979)', 6.415263624734343, 34020)\n",
            "('Reservoir Dogs (1992)', 6.205831000329978, 45318)\n",
            "('\"Shining', 6.175700739561374, 40297)\n",
            "('Requiem for a Dream (2000)', 6.041638315947007, 30402)\n",
            "('\"Good', 6.040596117829245, 23823)\n",
            "('Goodfellas (1990)', 6.035250005435119, 44592)\n",
            "('Eternal Sunshine of the Spotless Mind (2004)', 5.963101701998606, 46292)\n",
            "('Alien (1979)', 5.95763215351198, 46572)\n",
            "('Donnie Darko (2001)', 5.924413319259936, 36667)\n",
            "('Harry Potter and the Prisoner of Azkaban (2004)', 5.9078172758939225, 32517)\n",
            "(\"Monty Python's Life of Brian (1979)\", 5.906836046588124, 28801)\n",
            "('\"Shawshank Redemption', 5.903036156870099, 122296)\n",
            "('Kill Bill: Vol. 1 (2003)', 5.850071139395098, 46973)\n",
            "('Memento (2000)', 5.84109154513231, 55649)\n",
            "('Twelve Monkeys (a.k.a. 12 Monkeys) (1995)', 5.803686360726292, 59730)\n",
            "('\"Shawshank Redemption', 5.797215967440351, 122296)\n",
            "('Kill Bill: Vol. 2 (2004)', 5.736034165605361, 38553)\n",
            "('\"Usual Suspects', 5.719847850481825, 72893)\n",
            "(\"Schindler's List (1993)\", 5.688707424605917, 84232)\n",
            "('Casino (1995)', 5.650366171661123, 23932)\n",
            "('True Romance (1993)', 5.647379893249968, 18609)\n",
            "('\"Lord of the Rings: The Fellowship of the Ring', 5.640371689122126, 79940)\n",
            "('\"Lord of the Rings: The Two Towers', 5.640022303538952, 73687)\n",
            "('\"Shawshank Redemption', 5.638472554214329, 122296)\n",
            "('\"Godfather', 5.635259424038795, 75004)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "oxt9T6qy1PAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.recommendation import MatrixFactorizationModel\n",
        "\n",
        "model_path = \"/content/model\"\n",
        "\n",
        "# Save and load model\n",
        "model.save(sc, model_path)\n",
        "same_model = MatrixFactorizationModel.load(sc, model_path)"
      ],
      "metadata": {
        "id": "2O2-5pjB1QqW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/model.zip /content/model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JU7_Pml1yOx",
        "outputId": "f958b9b5-38d6-489e-f913-57146b7f0607"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/model/ (stored 0%)\n",
            "  adding: content/model/data/ (stored 0%)\n",
            "  adding: content/model/data/user/ (stored 0%)\n",
            "  adding: content/model/data/user/part-00000-3309dc45-0ebd-40d6-a714-bbdf11c690a5-c000.snappy.parquet (deflated 23%)\n",
            "  adding: content/model/data/user/part-00001-3309dc45-0ebd-40d6-a714-bbdf11c690a5-c000.snappy.parquet (deflated 23%)\n",
            "  adding: content/model/data/user/.part-00001-3309dc45-0ebd-40d6-a714-bbdf11c690a5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/model/data/user/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/model/data/user/_SUCCESS (stored 0%)\n",
            "  adding: content/model/data/user/.part-00000-3309dc45-0ebd-40d6-a714-bbdf11c690a5-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/model/data/product/ (stored 0%)\n",
            "  adding: content/model/data/product/.part-00000-fad96151-59d4-4b4c-9e77-eb94a2b0e959-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/model/data/product/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/model/data/product/part-00000-fad96151-59d4-4b4c-9e77-eb94a2b0e959-c000.snappy.parquet (deflated 23%)\n",
            "  adding: content/model/data/product/.part-00001-fad96151-59d4-4b4c-9e77-eb94a2b0e959-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: content/model/data/product/_SUCCESS (stored 0%)\n",
            "  adding: content/model/data/product/part-00001-fad96151-59d4-4b4c-9e77-eb94a2b0e959-c000.snappy.parquet (deflated 23%)\n",
            "  adding: content/model/metadata/ (stored 0%)\n",
            "  adding: content/model/metadata/.part-00000.crc (stored 0%)\n",
            "  adding: content/model/metadata/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/model/metadata/part-00000 (deflated 8%)\n",
            "  adding: content/model/metadata/_SUCCESS (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lLuZLqbJ197q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}